---
train_config:
  env_port: 19999
  seed: 1000
  exp_name: 's0'
  lr_rate: 0.0005
  w_decay: 0.00001
  restore: false # Restore for training or evaluation
  tps: 1800 # Restored episode for evaluation
  n_episode: 100 # Total Training Episode
  batch_size: 256 # Sample from PER
  update_step: 10 # Number of policy updates per env rollout
  mse_loss: true # for Q loss
  save_every: 1 # save and evaluate

  # Pretrain Setting
  pretrain_demo: false
  pretrain_step: 10000
  pretrain_save_step: 2000

agent_config:
  state_dim: 15
  action_dim: 1
  gamma: 0.9 # Reward discount
  seed: 54760
  replay_buffer_size: 1000000
  tau: 0.3 # Continuous update, source network portion, per episode
  action_noise_std: 0.1
  const_demo_priority: 0.99 # Should be less than 1
  const_min_priority: 0.001
  no_per: false # No prioritized experience replay (Don't  update priority)

demo_config:
  load_demo_data: true
  demo_dir: './demo/random'
  random: true
